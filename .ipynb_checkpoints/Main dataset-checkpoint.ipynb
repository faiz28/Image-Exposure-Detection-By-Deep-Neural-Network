{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b64de3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import MeanIoU, Recall, Precision, Accuracy\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam,schedules\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d880c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "TRAIN_SPLIT = 0.85\n",
    "\n",
    "DIR = './'\n",
    "imgH = 32\n",
    "imgW = 32\n",
    "\n",
    "# file = open('data_index.txt', 'w')\n",
    "count = 0\n",
    "\n",
    "def load_image_data():\n",
    "    imgDir = DIR + 'Normal/'\n",
    "    imgSet1 = prepare_image_array(imgDir, imgW, imgH)\n",
    "    m = imgSet1.shape[0]\n",
    "\n",
    "    imgDir = DIR + 'OverExposed/'\n",
    "    imgSet2 = prepare_image_array(imgDir, imgW, imgH)\n",
    "    n = imgSet2.shape[0]\n",
    "\n",
    "    imgDir = DIR + 'UnderExposed/'\n",
    "    imgSet3 = prepare_image_array(imgDir, imgW, imgH)\n",
    "    o = imgSet3.shape[0]\n",
    "\n",
    "    # Put all image data into one array.\n",
    "    imgSet = np.concatenate((imgSet1, imgSet2, imgSet3), axis=0)\n",
    "    print(imgSet.shape)\n",
    "\n",
    "    labelSet1 = np.zeros(m, dtype=np.uint8)\n",
    "    labelSet2 = np.ones(n, dtype=np.uint8)\n",
    "    label_o = np.ones(o, dtype=np.uint8)\n",
    "    labelSet3 = np.add(label_o, label_o)\n",
    "    labelSet = np.concatenate((labelSet1, labelSet2, labelSet3), axis=0)\n",
    "\n",
    "\n",
    "    p = imgSet.shape[0]  # p = n + m + o\n",
    "\n",
    "    # random.shuffle(indices)\n",
    "    # for i in range(len(indices)):\n",
    "    #     file.write(str(indices[i])+\",\")\n",
    "    \n",
    "    # file.close()\n",
    "    f = open('data_index.txt', 'r')\n",
    "    indices = []\n",
    "    for x in f.read().split(','):\n",
    "        if len(x) > 0:\n",
    "            num = ''\n",
    "            for j in x:\n",
    "                if(j>='0' and j<='9'):\n",
    "                    num +=j;\n",
    "            if len(num)>0 and  int(num)<p:\n",
    "                indices.append(int(num)) \n",
    "   \n",
    "    # print(indices)\n",
    "    imgSet = imgSet[indices]\n",
    "    labelSet = labelSet[indices]\n",
    "    print(labelSet[:5])\n",
    "    print(\"Label set : \")\n",
    "    labelSet = labelSet.reshape(p, 1)\n",
    "\n",
    "#   one hot encoding\n",
    "\n",
    "    labelSet = to_categorical(labelSet)\n",
    "\n",
    "    # print(labelSet[:5])\n",
    "    \n",
    "    # print(imgSet[0])\n",
    "    imgSet = imgSet.astype(np.float32)\n",
    "    imgSet = imgSet/255.0\n",
    "\n",
    "    r = int(p * TRAIN_SPLIT)\n",
    "    trainX = imgSet[:r]\n",
    "    trainY = labelSet[:r]\n",
    "    testX = imgSet[r:]\n",
    "    testY = labelSet[r:]\n",
    "\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "\n",
    "\n",
    "def prepare_image_array(imgDir, imgW, imgH):\n",
    "    imgList = os.listdir(imgDir)\n",
    "    # print(imgList)\n",
    "    n = len(imgList)\n",
    "\n",
    "    imgSet = []\n",
    "    for i in range(n):\n",
    "        imgPath = imgDir + imgList[i]\n",
    "        if (os.path.exists(imgPath)):\n",
    "            # print(imgPath)\n",
    "            img = cv2.imread(imgPath)\n",
    "            resizedImg = cv2.resize(img, (imgW, imgH))\n",
    "            rgbImg = cv2.cvtColor(resizedImg, cv2.COLOR_BGR2RGB)\n",
    "            imgSet.append(rgbImg)\n",
    "        else:\n",
    "            print(\"It is not a valid image path.\")\n",
    "\n",
    "    # print(\"total image \"+str(len(imgSet)))\n",
    "    imgSet = np.array(imgSet, dtype=np.uint8)\n",
    "    # print(\"total shape \"+str(imgSet.shape))\n",
    "\n",
    "    return imgSet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6a6bbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8984, 32, 32, 3)\n",
      "[0 2 1 2 0]\n",
      "Label set : \n"
     ]
    }
   ],
   "source": [
    "train_x,train_Y, testInSet, testOutSet =load_image.load_image_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb949431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def run_model(modelpath,testInSet, testOutSet):\n",
    "\n",
    "    lr_schedule = schedules.ExponentialDecay(\n",
    "        initial_learning_rate=0.01,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9)\n",
    "\n",
    "    sgd = SGD(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "    model = load_model(modelpath, compile = True)\n",
    "    model.compile(loss = 'mse', optimizer = sgd, metrics = ['accuracy', Recall(), Precision()])\n",
    "    \n",
    "\n",
    "\n",
    "#     y_test = np.argwhere(testOutSet)[:,1]\n",
    "    # print(y_test)\n",
    "#     indices = np.argwhere(y_test == 0).flatten()\n",
    "#     print(indices.shape)\n",
    "#     testInNormal = testInSet[indices]\n",
    "#     testOutNormal = testOutSet[indices]\n",
    "\n",
    "\n",
    "\n",
    "#     indices = np.argwhere(y_test == 1).flatten()\n",
    "#     testInOver = testInSet[indices]\n",
    "#     testOutOver = testOutSet[indices]\n",
    "#     indices = np.argwhere(y_test == 2).flatten()\n",
    "#     testInUnder = testInSet[indices]\n",
    "#     testOutUnder = testOutSet[indices]\n",
    "\n",
    "\n",
    "\n",
    "    '''\tTest Model. '''\n",
    "    lossT, accT, precisionT, recallT = model.evaluate(testInSet, testOutSet)\n",
    "#     lossN, accN, precisionN, recallN = model.evaluate(testInNormal, testOutNormal)\n",
    "#     lossO, accO, precisionO, recallO = model.evaluate(testInOver, testOutOver)\n",
    "#     lossU, accU, precisionU, recallU = model.evaluate(testInUnder, testOutUnder)\n",
    "\n",
    "\n",
    "    print(\"\\n\\nlossT = %s, accT = %s,  precisionT = %s, recallT = %s \"%(lossT, accT, precisionT, recallT))\n",
    "#     print(\"lossN = %s, accN = %s,  precisionN = %s, recallN = %s \"%(lossN, accN, precisionN, recallN))\n",
    "#     print(\"lossO = %s, accO = %s,  precisionO = %s, recallO = %s \"%(lossO, accO, precisionO, recallO))\n",
    "#     print(\"lossU = %s, accU = %s,  precisionU = %s, recallU = %s \"%(lossU, accU, precisionU, recallU))\n",
    "\n",
    "    predicted_output = []\n",
    "    prediction = model.predict(testInSet)\n",
    "    for i in prediction:\n",
    "        predicted_output.append(np.argmax(i)) \n",
    "\n",
    "    actual_output = []\n",
    "    for i in testOutSet:\n",
    "        actual_output.append(np.argmax(i)) \n",
    "    con_mat = confusion_matrix(actual_output, predicted_output)\n",
    "    for i in con_mat:\n",
    "        print(i)\n",
    "        \n",
    "    cm =  classification_report(con_mat )\n",
    "    print(cm)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a56a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Non trainable\")\n",
    "DIR = './non-trainable/'\n",
    "modelpath = DIR + 'VGG_Classifier.hdf5'\n",
    "run_model(modelpath,testInSet, testOutSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "65956609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable\n",
      "43/43 [==============================] - 7s 152ms/step - loss: 0.1670 - accuracy: 0.6350 - recall_67: 0.4429 - precision_67: 0.7654\n",
      "\n",
      "\n",
      "lossT = 0.16703175008296967, accT = 0.6350148320198059,  precisionT = 0.4428783357143402, recallT = 0.7653846144676208 \n",
      "[114 157 150]\n",
      "[ 73 329  50]\n",
      "[ 45  17 413]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.27      0.35       421\n",
      "           1       0.65      0.73      0.69       452\n",
      "           2       0.67      0.87      0.76       475\n",
      "\n",
      "    accuracy                           0.64      1348\n",
      "   macro avg       0.61      0.62      0.60      1348\n",
      "weighted avg       0.61      0.64      0.61      1348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"trainable\")\n",
    "DIR = './trainable/'\n",
    "modelpath = DIR + 'trainable-tf-Classifier.hdf5'\n",
    "run_model(modelpath,testInSet, testOutSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a79cdc90",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (1879539685.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [96]\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"fully connected\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"fully connected \")\n",
    "DIR = './Fully-Connnected-model/'\n",
    "modelpath = DIR + 'Fully_connected_Classifier.hdf5'\n",
    "run_model(modelpath,testInSet, testOutSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "493c74bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smalll\n",
      "43/43 [==============================] - 1s 11ms/step - loss: 0.1054 - accuracy: 0.7834 - recall_68: 0.7678 - precision_68: 0.7865\n",
      "\n",
      "\n",
      "lossT = 0.10542751103639603, accT = 0.783382773399353,  precisionT = 0.7678041458129883, recallT = 0.7864741683006287 \n",
      "[288  67  66]\n",
      "[ 75 361  16]\n",
      "[ 67   1 407]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.68       421\n",
      "           1       0.84      0.80      0.82       452\n",
      "           2       0.83      0.86      0.84       475\n",
      "\n",
      "    accuracy                           0.78      1348\n",
      "   macro avg       0.78      0.78      0.78      1348\n",
      "weighted avg       0.78      0.78      0.78      1348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"smalll\")\n",
    "DIR = './Small-Size-Convolution-Maxpooling-model/'\n",
    "modelpath = DIR + 'Small_CNN_Classifier.hdf5'\n",
    "run_model(modelpath,testInSet, testOutSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0cc3d879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large-Size-Convolution-Maxpooling-model\n",
      "43/43 [==============================] - 3s 57ms/step - loss: 0.1594 - accuracy: 0.7211 - recall_69: 0.3909 - precision_69: 0.9039\n",
      "\n",
      "\n",
      "lossT = 0.15939196944236755, accT = 0.721068263053894,  precisionT = 0.390949547290802, recallT = 0.9039450883865356 \n",
      "[144 184  93]\n",
      "[ 31 405  16]\n",
      "[ 43   9 423]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.34      0.45       421\n",
      "           1       0.68      0.90      0.77       452\n",
      "           2       0.80      0.89      0.84       475\n",
      "\n",
      "    accuracy                           0.72      1348\n",
      "   macro avg       0.71      0.71      0.69      1348\n",
      "weighted avg       0.71      0.72      0.70      1348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Large-Size-Convolution-Maxpooling-model\")\n",
    "DIR = './Large-Size-Convolution-Maxpooling-model/'\n",
    "modelpath = DIR + 'Large_CNN_Classifier.hdf5'\n",
    "run_model(modelpath,testInSet, testOutSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad653b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97d436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1b349d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ddc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Large-Size-Convolution-Maxpooling-model\")\n",
    "DIR = './Large-Size-Convolution-Maxpooling-model/'\n",
    "modelpath = DIR + 'Large_CNN_Classifier.hdf5'\n",
    "run_model(modelpath,testInSet, testOutSet)a  = {}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
